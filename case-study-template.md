# Case-study оптимизации, часть 2 [(часть 1)](https://github.com/KirkovAlexey/task-1/blob/master/case-study-template.md)

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику:

- Измерять количество потребляймой памяти
- Измерять количество затраченного времени

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста позволяет не допустить изменения логики программы при оптимизации.

## Асимптотика

Для упрощения работы с данными подготовил программу `preparing-data.rb`, которая генерит данные нескольких размеров (1mb, 4Mb, 10Mb и оригинальный файл) из архива.

Для просмотра метрик я использую программу `asymptotics.rb`, которая замеряет метрики для данных разного объема.

До начала оптимизации были следующие параметры

``` shellsession

Calculating -------------------------------------
      3.658  (± 1.7%) i/s -     19.000  in   5.200075s
      0.938  (± 3.6%) i/s -      5.000  in   5.337153s
      0.379  (± 3.2%) i/s -      2.000  in   5.289088s
                   with 99.0% confidence

Comparison:
         Process 1Mb:        3.7 i/s
         Process 4Mb:        0.9 i/s - 3.90x  (± 0.16) slower
        Process 10Mb:        0.4 i/s - 9.66x  (± 0.44) slower
                   with 99.0% confidence

```

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за *время, которое у вас получилось*

Вот как я построил `feedback_loop`:

- Используя асимптотику я выбрал минимальный размер файла 1 Mb, который позволял успешно выполится программе без оптимизации
- Использований метрик по времени выполения и памяти
- Поиск возможных точек роста
- Оптимизация кода
- Повторный запуск программы и анализ метрик

## Вникаем в детали системы, чтобы найти 20% точек роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался следующими инструментами:

`benchmark`, `memory_profiler`, `ruby-prof`, `benchmark-ips`, `valgrind`

Вот какие проблемы удалось найти и решить

``` shellsession

%self      total      self      wait     child     calls  name
16.15      0.354     0.068     0.000     0.286        3   Array#each
10.94      0.046     0.046     0.000     0.000    22614   Array#include?
10.81      0.045     0.045     0.000     0.000    26693   String#split
10.33      0.047     0.043     0.000     0.004    22614   <Class::Date>#strptime
 7.64      0.138     0.032     0.000     0.106    22614   Object#parse_session
 7.23      0.057     0.030     0.000     0.027        1   JSON::Ext::Generator::GeneratorMethods::Hash#to_json
 4.98      0.021     0.021     0.000     0.000    67478   String#encode
 3.58      0.015     0.015     0.000     0.000    22614   Date#iso8601
 3.45      0.014     0.014     0.000     0.000     8156   Array#map
 2.56      0.011     0.011     0.000     0.000    17028   String#=~
 2.34      0.010     0.010     0.000     0.000    68242   Array#last
 2.08      0.009     0.009     0.000     0.000     8157   Array#sort
```

Метрика была получена с помощью `ruby-prof` в режиме `FlatPrinter`

### Ваша находка №1

Аналируя метрику я увидел, что на четвертой строчке стоит парсинг даты.
В прошлой части я уже изменил парсинг даты с `Date.parse` на `Date.strptime`, это принесло результат.
В этот раз я обнаружил, что формат даты изначально совпадает с форматом `iso8601` и это означает что возможно удалить избытоное преобразование.

Метод после изменения:

``` shellsession
def parse_session(session)
  fields = session.split(COMMA)
  {
    'user_id' => fields[1],
    'session_id' => fields[2],
    'browser' => fields[3].upcase,
    'time' => fields[4].to_i,
    'date' => fields[5]
  }
end
```

После удаления избыточного преобразования время выполнения метода `parse_session` уменьшилось.
Вместо 32.72% стало 21.71% из 100% времени работы скрипта.

Профилировка выполнялась помощью `ruby-prof` в режиме `GraphHtmlPrinter`

``` shellsession
32.72%	7.62%	0.14	0.03	0.00	0.10	22614	Object#parse_session	32
           	 	0.04	0.04	0.00	0.00	22614/22614	<Class::Date>#strptime	34
           	 	0.03	0.03	0.00	0.00	22614/26693	String#split	32
           	 	0.02	0.02	0.00	0.00	22614/22614	Date#iso8601	34
           	 	0.01	0.01	0.00	0.00	22614/22614	String#upcase	34
           	 	0.01	0.01	0.00	0.00	22614/22614	String#to_i 34
```

``` shellsession
21.71%	7.04%	0.07	0.02	0.00	0.05	22614	Object#parse_session	32
           	 	0.04	0.04	0.00	0.00	22614/26693	String#split	32
           	 	0.01	0.01	0.00	0.00	22614/22614	String#upcase	34
           	 	0.00	0.00	0.00	0.00	22614/22614	String#to_i	34
```

### Ваша находка №2

По отчету видно что второе место по затратам занимает метод `Array#include?`, который используется при формировании массива с сессиями юзеров.
При анализе исходного кода видно, что мы можем использовать `SortedSet`, так как это нам даст выигрыш в скорости при поиске элемента в массиве и не будет необходимости сортировать массив для статистики.

Замеры при помощи `ruby-prof` в режиме `GraphHtmlPrinter` показали прирост времени выполения с `0.33529114723205566` до `0.31107091903686523`
Затраченное время также сократилось с `0.05` до `0.01` для `Array#include?` и `Set#include?` соотвественно.

### Ваша находка №3

Пятое место при профилировании с помощью `RubyProf::CPU_TIME` занимает преобразование `hash` в `json`.
Это можно оптимизировать с помощью замены библиотеки на более быструю.
До этого я уже сталкивался с такой проблемой и знаю, что самая быстрая библиотека это `Oj`.
Будем использовать ее и записывать сразу в файл результат `Oj.to_file`

В итоге скорость изменилась с

``` shellsession
18.05%	9.01%	0.26	0.13	0.00	0.13	1	JSON::Ext::Generator::GeneratorMethods::Hash#to_json
```

на

``` shellsession
0.55%	0.55%	0.01	0.01	0.00	0.00	1	<Module::Oj>#to_file
```

Общее время также уменьшилось с `1.449656` до `1.1936809999999998`

## Результаты
Я также пробовал заменить `each` на `while`, но это давало грошевую оптимизацию и не приносило ощутимой разницы в метрике.


## Защита от регресса производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы был добавлен тест регрессии по времени
