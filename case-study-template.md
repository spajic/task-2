# Case-study оптимизация. Часть 2.

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: объем затрачиваемой памяти и время выполнения

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста позволяет не допустить изменения логики программы при оптимизации.

## Асимптотика
До оптимизации:
```
Calculating -------------------------------------
         File: 512Kb      4.817  (± 5.3%) i/s -     24.000  in   5.032638s
           File: 1Mb      2.527  (± 6.3%) i/s -     13.000  in   5.183298s
           File: 2Mb      1.203  (±15.4%) i/s -      6.000  in   5.132240s
           File: 4Mb      0.586  (±19.5%) i/s -      3.000  in   5.269748s
           File: 8Mb      0.287  (±14.0%) i/s -      2.000  in   7.100129s
                   with 99.0% confidence

Comparison:
         File: 512Kb:        4.8 i/s
           File: 1Mb:        2.5 i/s - 1.91x  (± 0.15) slower
           File: 2Mb:        1.2 i/s - 4.00x  (± 0.68) slower
           File: 4Mb:        0.6 i/s - 8.18x  (± 1.96) slower
           File: 8Mb:        0.3 i/s - 16.77x  (± 3.11) slower

```
Исходя из данных показателей мы видим, что увеличение объема обрабатываемых данных в 2 раза ведет к росту времени работы в 2 раза

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений в среднем за 10 секунд

Вот как я построил `feedback_loop`:
1. Изначально выбрал маленький размер исходных данных (около 1Мб) позволяющий скрипту успешно отработать без оптимизаций
2. Поиск базовой метрики (время и память)
3. Дописал тест на регрессию по времени и памяти
4. Профилирование и поиск "точек роста"
5. Внесение изменений в код
6. Повторное тестирование и сбор новых метрик
7. Увеличение объема данных

## Вникаем в детали системы, чтобы найти 20% точек роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался

Гемы:
* ruby-prof
* memory_profiler
* get_process_mem
* benchmark-ips
Stdlib:
* benchmark
C
* valgrind (tool - massif)

Вот какие проблемы удалось найти и решить
Согласно результатам ruby-prof в режиме cpu:
```
 %self      total      self      wait     child     calls  name
 23.99      9.674     2.918     0.000     6.756        1   IO#each_line
  6.89      1.007     0.838     0.000     0.169    48639   Array#map
  6.74      1.413     0.819     0.000     0.594    89009   <Class::Date>#iso8601
  6.71      2.488     0.816     0.000     1.672        1   JSON::Ext::Generator::GeneratorMethods::Hash#to_json
  5.94      2.302     0.722     0.000     1.581    89009   Object#parse_session
  5.64      3.431     0.685     0.000     2.746    16214   Object#aggregate_user_stats
```
### Находка №1
Проанализировав исходные данные выяснилось, что даты изначально приходят в нужном формате, поэтому нет нужды создавать Date объект
Также были убраны неиспользуемые поля, а вместо полей first_name и last_name было сделано поле full_name.
```
Comparison:
         File: 512Kb:       10.9 i/s
           File: 1Mb:        5.9 i/s - 1.85x  (± 0.14) slower
           File: 2Mb:        2.8 i/s - 3.94x  (± 0.57) slower
           File: 4Mb:        1.4 i/s - 7.90x  (± 1.57) slower
           File: 8Mb:        0.6 i/s - 16.93x  (± 5.53) slower
                   with 99.0% confidence
```
Фактически мы видим двукратный прирост производительности

### Находка №2
Также стандартная json библиотека не самая производительная заменив ее на gem oj, мы получили еще небольшой прирост:
```
Comparison:
         File: 512Kb:       12.5 i/s
           File: 1Mb:        7.0 i/s - 1.78x  (± 0.12) slower
           File: 2Mb:        3.4 i/s - 3.68x  (± 0.42) slower
           File: 4Mb:        1.6 i/s - 7.71x  (± 1.27) slower
           File: 8Mb:        0.8 i/s - 15.36x  (± 3.33) slower
                   with 99.0% confidence
```

Однако нерешенной остается проблема резкого выделения [памяти](https://imgur.com/cvkmvMS) связанная с формирование json строки

### Находка №3
Вместо библиотеки, которая целиком парсит hash в json, можно использовать yajl, которая позволяет "стримить" json в файл
Мы немного проигрываем в производительности на маленьких файлах
```
Comparison:
         File: 512Kb:       12.3 i/s
           File: 1Mb:        6.8 i/s - 1.81x  (± 0.13) slower
           File: 2Mb:        3.3 i/s - 3.67x  (± 0.33) slower
           File: 4Mb:        1.6 i/s - 7.56x  (± 1.22) slower
           File: 8Mb:        0.8 i/s - 16.21x  (± 5.33) slower
                   with 99.0% confidence
```
Однако мы существенно сокращаем объемы потребляемой ![памяти](https://imgur.com/pzdL2kW)

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
В среднем файл обрабатывается за 28 - 29 секунд при средних затратах по памяти 600 - 620Мб без резких скачков по памяти

## Защита от регресса производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы добавлены дополнительные тесты для защиты от регрессий по памяти и времени выполнения
